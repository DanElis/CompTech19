{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trushev_conv_dense.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8ZcgflGzUaXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c75ed4e3-91e9-4f09-8020-c3facb4a7b58"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FMWHbPp9UaXw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0e3eacdd-5fc9-4b2e-ecf7-a1b974734b4c"
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from keras.utils import to_categorical \n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "\n",
        "path = '/content/gdrive/My Drive/Colab Notebooks/winter_school_2019/seismic_images/Lapteva_faults_10k.pickle'\n",
        "with open(path, 'rb') as handle:\n",
        "    data = pickle.load(handle)\n",
        "    \n",
        "X = data['X_set'] \n",
        "y = data['y_set'] \n",
        "X = X.reshape(X.shape + (1,))\n",
        "y = y.reshape(y.shape + (1,))\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.15, random_state=25)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=2) \n",
        "y_test = to_categorical(y_test, num_classes=2)\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10240, 64, 64, 1) (10240, 64, 64, 1)\n",
            "(8704, 64, 64, 1) (8704, 64, 64, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hckD7lzjUaX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "7926dce7-1a1d-4e90-cae3-7239954a5ba9"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "\n",
        "from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, BatchNormalization, Dropout, MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    weights = K.variable(weights)\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def create_model(\n",
        "        depth=4,\n",
        "        batch_norm=.3,\n",
        "        dropout=.3,\n",
        "        pooling=False,\n",
        "        activation='relu',\n",
        "        activation_out='sigmoid',\n",
        "        lr=.001,\n",
        "        weight_false=.2,\n",
        "        weight_true=1,\n",
        "):\n",
        "    x = input_img = Input(shape=(64, 64, 1))\n",
        "\n",
        "    filters = 16\n",
        "    for i in range(depth):      \n",
        "      x = Conv2D(filters, (3, 3), activation=activation, padding='same')(x)\n",
        "      x = BatchNormalization()(x) if batch_norm else x\n",
        "      x = Dropout(dropout)(x) if dropout else x\n",
        "      x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x) if pooling else x\n",
        "      \n",
        "      filters = filters * 2 if filters < 64 else filters\n",
        "        \n",
        "    x = Flatten()(x)    \n",
        "    x = Dense(100, activation=activation)(x)    \n",
        "    x = Dropout(dropout)(x) if dropout else x\n",
        "    x = Dense(64 * 64 * 2, activation=activation_out)(x)\n",
        "    x = Reshape((64, 64, 2), input_shape=(64 * 64 * 2,))(x)\n",
        "    \n",
        "    model = Model(input_img, x, name=\"classification\")\n",
        "    loss = weighted_categorical_crossentropy(np.array([weight_false, weight_true]))\n",
        "    model.compile(\n",
        "        optimizer=Adam(lr=lr),\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 64, 64, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64, 64, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 64, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 262144)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               26214500  \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8192)              827392    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 64, 64, 2)         0         \n",
            "=================================================================\n",
            "Total params: 27,102,820\n",
            "Trainable params: 27,102,468\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FoG08Ziexptw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "D--_ibl9UaYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5066
        },
        "outputId": "0be1583a-86da-44fd-f521-84f40c02d087"
      },
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=200,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    validation_split=.3,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6092 samples, validate on 2612 samples\n",
            "Epoch 1/200\n",
            "6092/6092 [==============================] - 21s 4ms/step - loss: 0.0936 - acc: 0.9283 - val_loss: 0.0819 - val_acc: 0.9619\n",
            "Epoch 2/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0804 - acc: 0.9605 - val_loss: 0.0802 - val_acc: 0.9620\n",
            "Epoch 3/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0716 - acc: 0.9605 - val_loss: 0.0860 - val_acc: 0.9614\n",
            "Epoch 4/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0643 - acc: 0.9600 - val_loss: 0.0701 - val_acc: 0.9620\n",
            "Epoch 5/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0587 - acc: 0.9598 - val_loss: 0.0671 - val_acc: 0.9617\n",
            "Epoch 6/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0548 - acc: 0.9590 - val_loss: 0.0687 - val_acc: 0.9614\n",
            "Epoch 7/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0522 - acc: 0.9588 - val_loss: 0.0685 - val_acc: 0.9615\n",
            "Epoch 8/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0501 - acc: 0.9585 - val_loss: 0.0627 - val_acc: 0.9608\n",
            "Epoch 9/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0486 - acc: 0.9581 - val_loss: 0.0658 - val_acc: 0.9609\n",
            "Epoch 10/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0471 - acc: 0.9578 - val_loss: 0.0690 - val_acc: 0.9613\n",
            "Epoch 11/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0463 - acc: 0.9579 - val_loss: 0.0676 - val_acc: 0.9613\n",
            "Epoch 12/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0456 - acc: 0.9579 - val_loss: 0.0648 - val_acc: 0.9606\n",
            "Epoch 13/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0444 - acc: 0.9578 - val_loss: 0.0677 - val_acc: 0.9611\n",
            "Epoch 14/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0439 - acc: 0.9581 - val_loss: 0.0652 - val_acc: 0.9610\n",
            "Epoch 15/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0434 - acc: 0.9580 - val_loss: 0.0644 - val_acc: 0.9608\n",
            "Epoch 16/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0427 - acc: 0.9584 - val_loss: 0.0638 - val_acc: 0.9601\n",
            "Epoch 17/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0424 - acc: 0.9580 - val_loss: 0.0646 - val_acc: 0.9606\n",
            "Epoch 18/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0417 - acc: 0.9583 - val_loss: 0.0672 - val_acc: 0.9612\n",
            "Epoch 19/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0414 - acc: 0.9585 - val_loss: 0.0651 - val_acc: 0.9610\n",
            "Epoch 20/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0408 - acc: 0.9584 - val_loss: 0.0654 - val_acc: 0.9608\n",
            "Epoch 21/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0402 - acc: 0.9589 - val_loss: 0.0663 - val_acc: 0.9610\n",
            "Epoch 22/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0399 - acc: 0.9587 - val_loss: 0.0657 - val_acc: 0.9614\n",
            "Epoch 23/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0398 - acc: 0.9588 - val_loss: 0.0659 - val_acc: 0.9617\n",
            "Epoch 24/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0391 - acc: 0.9591 - val_loss: 0.0661 - val_acc: 0.9617\n",
            "Epoch 25/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0386 - acc: 0.9593 - val_loss: 0.0661 - val_acc: 0.9616\n",
            "Epoch 26/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0384 - acc: 0.9592 - val_loss: 0.0654 - val_acc: 0.9616\n",
            "Epoch 27/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0380 - acc: 0.9593 - val_loss: 0.0685 - val_acc: 0.9622\n",
            "Epoch 28/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0377 - acc: 0.9599 - val_loss: 0.0667 - val_acc: 0.9622\n",
            "Epoch 29/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0373 - acc: 0.9597 - val_loss: 0.0669 - val_acc: 0.9624\n",
            "Epoch 30/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0370 - acc: 0.9596 - val_loss: 0.0675 - val_acc: 0.9624\n",
            "Epoch 31/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0370 - acc: 0.9601 - val_loss: 0.0656 - val_acc: 0.9620\n",
            "Epoch 32/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0366 - acc: 0.9601 - val_loss: 0.0680 - val_acc: 0.9626\n",
            "Epoch 33/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0363 - acc: 0.9602 - val_loss: 0.0658 - val_acc: 0.9624\n",
            "Epoch 34/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0358 - acc: 0.9604 - val_loss: 0.0657 - val_acc: 0.9623\n",
            "Epoch 35/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0356 - acc: 0.9603 - val_loss: 0.0671 - val_acc: 0.9627\n",
            "Epoch 36/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0354 - acc: 0.9606 - val_loss: 0.0703 - val_acc: 0.9633\n",
            "Epoch 37/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0352 - acc: 0.9606 - val_loss: 0.0677 - val_acc: 0.9630\n",
            "Epoch 38/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0350 - acc: 0.9606 - val_loss: 0.0664 - val_acc: 0.9627\n",
            "Epoch 39/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0348 - acc: 0.9609 - val_loss: 0.0640 - val_acc: 0.9626\n",
            "Epoch 40/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0346 - acc: 0.9605 - val_loss: 0.0676 - val_acc: 0.9632\n",
            "Epoch 41/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0344 - acc: 0.9606 - val_loss: 0.0684 - val_acc: 0.9633\n",
            "Epoch 42/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0340 - acc: 0.9611 - val_loss: 0.0645 - val_acc: 0.9624\n",
            "Epoch 43/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0340 - acc: 0.9606 - val_loss: 0.0662 - val_acc: 0.9631\n",
            "Epoch 44/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0338 - acc: 0.9610 - val_loss: 0.0672 - val_acc: 0.9633\n",
            "Epoch 45/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0335 - acc: 0.9609 - val_loss: 0.0678 - val_acc: 0.9636\n",
            "Epoch 46/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0334 - acc: 0.9610 - val_loss: 0.0639 - val_acc: 0.9627\n",
            "Epoch 47/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0332 - acc: 0.9613 - val_loss: 0.0640 - val_acc: 0.9628\n",
            "Epoch 48/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0329 - acc: 0.9612 - val_loss: 0.0673 - val_acc: 0.9634\n",
            "Epoch 49/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0330 - acc: 0.9611 - val_loss: 0.0666 - val_acc: 0.9636\n",
            "Epoch 50/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0328 - acc: 0.9613 - val_loss: 0.0662 - val_acc: 0.9631\n",
            "Epoch 51/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0326 - acc: 0.9613 - val_loss: 0.0687 - val_acc: 0.9639\n",
            "Epoch 52/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0325 - acc: 0.9616 - val_loss: 0.0677 - val_acc: 0.9637\n",
            "Epoch 53/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0323 - acc: 0.9616 - val_loss: 0.0696 - val_acc: 0.9637\n",
            "Epoch 54/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0323 - acc: 0.9614 - val_loss: 0.0642 - val_acc: 0.9630\n",
            "Epoch 55/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0320 - acc: 0.9618 - val_loss: 0.0679 - val_acc: 0.9636\n",
            "Epoch 56/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0319 - acc: 0.9615 - val_loss: 0.0691 - val_acc: 0.9638\n",
            "Epoch 57/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0319 - acc: 0.9616 - val_loss: 0.0700 - val_acc: 0.9640\n",
            "Epoch 58/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0316 - acc: 0.9620 - val_loss: 0.0654 - val_acc: 0.9633\n",
            "Epoch 59/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0315 - acc: 0.9620 - val_loss: 0.0619 - val_acc: 0.9627\n",
            "Epoch 60/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0316 - acc: 0.9619 - val_loss: 0.0707 - val_acc: 0.9642\n",
            "Epoch 61/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0316 - acc: 0.9616 - val_loss: 0.0673 - val_acc: 0.9637\n",
            "Epoch 62/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0314 - acc: 0.9620 - val_loss: 0.0658 - val_acc: 0.9638\n",
            "Epoch 63/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0312 - acc: 0.9620 - val_loss: 0.0699 - val_acc: 0.9643\n",
            "Epoch 64/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0315 - acc: 0.9615 - val_loss: 0.0702 - val_acc: 0.9642\n",
            "Epoch 65/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0312 - acc: 0.9618 - val_loss: 0.0635 - val_acc: 0.9633\n",
            "Epoch 66/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0309 - acc: 0.9621 - val_loss: 0.0706 - val_acc: 0.9645\n",
            "Epoch 67/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0308 - acc: 0.9621 - val_loss: 0.0681 - val_acc: 0.9640\n",
            "Epoch 68/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0309 - acc: 0.9622 - val_loss: 0.0669 - val_acc: 0.9639\n",
            "Epoch 69/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0307 - acc: 0.9622 - val_loss: 0.0678 - val_acc: 0.9640\n",
            "Epoch 70/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0306 - acc: 0.9622 - val_loss: 0.0656 - val_acc: 0.9637\n",
            "Epoch 71/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0305 - acc: 0.9624 - val_loss: 0.0655 - val_acc: 0.9637\n",
            "Epoch 72/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0304 - acc: 0.9622 - val_loss: 0.0694 - val_acc: 0.9642\n",
            "Epoch 73/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0304 - acc: 0.9621 - val_loss: 0.0686 - val_acc: 0.9645\n",
            "Epoch 74/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0304 - acc: 0.9623 - val_loss: 0.0679 - val_acc: 0.9640\n",
            "Epoch 75/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0303 - acc: 0.9622 - val_loss: 0.0659 - val_acc: 0.9639\n",
            "Epoch 76/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0300 - acc: 0.9625 - val_loss: 0.0685 - val_acc: 0.9640\n",
            "Epoch 77/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0301 - acc: 0.9624 - val_loss: 0.0676 - val_acc: 0.9642\n",
            "Epoch 78/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0299 - acc: 0.9625 - val_loss: 0.0724 - val_acc: 0.9649\n",
            "Epoch 79/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0296 - acc: 0.9627 - val_loss: 0.0668 - val_acc: 0.9641\n",
            "Epoch 80/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0299 - acc: 0.9626 - val_loss: 0.0674 - val_acc: 0.9638\n",
            "Epoch 81/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0297 - acc: 0.9627 - val_loss: 0.0679 - val_acc: 0.9642\n",
            "Epoch 82/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0298 - acc: 0.9626 - val_loss: 0.0690 - val_acc: 0.9644\n",
            "Epoch 83/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0294 - acc: 0.9631 - val_loss: 0.0678 - val_acc: 0.9642\n",
            "Epoch 84/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0296 - acc: 0.9627 - val_loss: 0.0664 - val_acc: 0.9642\n",
            "Epoch 85/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0293 - acc: 0.9630 - val_loss: 0.0709 - val_acc: 0.9647\n",
            "Epoch 86/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0292 - acc: 0.9631 - val_loss: 0.0674 - val_acc: 0.9641\n",
            "Epoch 87/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0292 - acc: 0.9630 - val_loss: 0.0727 - val_acc: 0.9649\n",
            "Epoch 88/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0294 - acc: 0.9627 - val_loss: 0.0705 - val_acc: 0.9647\n",
            "Epoch 89/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0291 - acc: 0.9631 - val_loss: 0.0690 - val_acc: 0.9647\n",
            "Epoch 90/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0289 - acc: 0.9634 - val_loss: 0.0668 - val_acc: 0.9644\n",
            "Epoch 91/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0292 - acc: 0.9626 - val_loss: 0.0716 - val_acc: 0.9648\n",
            "Epoch 92/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0289 - acc: 0.9634 - val_loss: 0.0687 - val_acc: 0.9642\n",
            "Epoch 93/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0289 - acc: 0.9631 - val_loss: 0.0704 - val_acc: 0.9648\n",
            "Epoch 94/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0287 - acc: 0.9634 - val_loss: 0.0699 - val_acc: 0.9646\n",
            "Epoch 95/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0287 - acc: 0.9634 - val_loss: 0.0701 - val_acc: 0.9645\n",
            "Epoch 96/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0287 - acc: 0.9634 - val_loss: 0.0673 - val_acc: 0.9641\n",
            "Epoch 97/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0288 - acc: 0.9632 - val_loss: 0.0730 - val_acc: 0.9652\n",
            "Epoch 98/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0286 - acc: 0.9635 - val_loss: 0.0668 - val_acc: 0.9645\n",
            "Epoch 99/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0287 - acc: 0.9634 - val_loss: 0.0730 - val_acc: 0.9653\n",
            "Epoch 100/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0283 - acc: 0.9637 - val_loss: 0.0733 - val_acc: 0.9654\n",
            "Epoch 101/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0285 - acc: 0.9635 - val_loss: 0.0678 - val_acc: 0.9646\n",
            "Epoch 102/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0284 - acc: 0.9635 - val_loss: 0.0675 - val_acc: 0.9646\n",
            "Epoch 103/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0283 - acc: 0.9637 - val_loss: 0.0673 - val_acc: 0.9647\n",
            "Epoch 104/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0283 - acc: 0.9636 - val_loss: 0.0724 - val_acc: 0.9652\n",
            "Epoch 105/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0283 - acc: 0.9636 - val_loss: 0.0741 - val_acc: 0.9656\n",
            "Epoch 106/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0283 - acc: 0.9635 - val_loss: 0.0714 - val_acc: 0.9649\n",
            "Epoch 107/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0281 - acc: 0.9638 - val_loss: 0.0712 - val_acc: 0.9653\n",
            "Epoch 108/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0282 - acc: 0.9637 - val_loss: 0.0732 - val_acc: 0.9653\n",
            "Epoch 109/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0283 - acc: 0.9636 - val_loss: 0.0750 - val_acc: 0.9654\n",
            "Epoch 110/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0280 - acc: 0.9637 - val_loss: 0.0697 - val_acc: 0.9649\n",
            "Epoch 111/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0279 - acc: 0.9638 - val_loss: 0.0718 - val_acc: 0.9651\n",
            "Epoch 112/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0281 - acc: 0.9636 - val_loss: 0.0713 - val_acc: 0.9649\n",
            "Epoch 113/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0277 - acc: 0.9638 - val_loss: 0.0714 - val_acc: 0.9651\n",
            "Epoch 114/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0277 - acc: 0.9641 - val_loss: 0.0720 - val_acc: 0.9651\n",
            "Epoch 115/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0276 - acc: 0.9643 - val_loss: 0.0709 - val_acc: 0.9650\n",
            "Epoch 116/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0279 - acc: 0.9638 - val_loss: 0.0700 - val_acc: 0.9648\n",
            "Epoch 117/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0278 - acc: 0.9641 - val_loss: 0.0690 - val_acc: 0.9648\n",
            "Epoch 118/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0276 - acc: 0.9639 - val_loss: 0.0692 - val_acc: 0.9648\n",
            "Epoch 119/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0278 - acc: 0.9637 - val_loss: 0.0713 - val_acc: 0.9652\n",
            "Epoch 120/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0276 - acc: 0.9640 - val_loss: 0.0712 - val_acc: 0.9650\n",
            "Epoch 121/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0275 - acc: 0.9641 - val_loss: 0.0703 - val_acc: 0.9651\n",
            "Epoch 122/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0276 - acc: 0.9640 - val_loss: 0.0666 - val_acc: 0.9647\n",
            "Epoch 123/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0276 - acc: 0.9640 - val_loss: 0.0726 - val_acc: 0.9652\n",
            "Epoch 124/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0274 - acc: 0.9642 - val_loss: 0.0724 - val_acc: 0.9653\n",
            "Epoch 125/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0273 - acc: 0.9643 - val_loss: 0.0689 - val_acc: 0.9647\n",
            "Epoch 126/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0274 - acc: 0.9642 - val_loss: 0.0730 - val_acc: 0.9654\n",
            "Epoch 127/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0272 - acc: 0.9644 - val_loss: 0.0684 - val_acc: 0.9647\n",
            "Epoch 128/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0273 - acc: 0.9642 - val_loss: 0.0735 - val_acc: 0.9654\n",
            "Epoch 129/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0273 - acc: 0.9641 - val_loss: 0.0732 - val_acc: 0.9653\n",
            "Epoch 130/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0272 - acc: 0.9644 - val_loss: 0.0686 - val_acc: 0.9646\n",
            "Epoch 131/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0272 - acc: 0.9642 - val_loss: 0.0746 - val_acc: 0.9654\n",
            "Epoch 132/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0269 - acc: 0.9645 - val_loss: 0.0719 - val_acc: 0.9652\n",
            "Epoch 133/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0271 - acc: 0.9644 - val_loss: 0.0767 - val_acc: 0.9658\n",
            "Epoch 134/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0271 - acc: 0.9644 - val_loss: 0.0735 - val_acc: 0.9655\n",
            "Epoch 135/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0269 - acc: 0.9646 - val_loss: 0.0723 - val_acc: 0.9655\n",
            "Epoch 136/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9647 - val_loss: 0.0747 - val_acc: 0.9656\n",
            "Epoch 137/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9648 - val_loss: 0.0748 - val_acc: 0.9654\n",
            "Epoch 138/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0268 - acc: 0.9646 - val_loss: 0.0792 - val_acc: 0.9660\n",
            "Epoch 139/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0269 - acc: 0.9647 - val_loss: 0.0694 - val_acc: 0.9647\n",
            "Epoch 140/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0268 - acc: 0.9646 - val_loss: 0.0721 - val_acc: 0.9653\n",
            "Epoch 141/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9647 - val_loss: 0.0689 - val_acc: 0.9646\n",
            "Epoch 142/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0266 - acc: 0.9648 - val_loss: 0.0766 - val_acc: 0.9657\n",
            "Epoch 143/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9646 - val_loss: 0.0701 - val_acc: 0.9650\n",
            "Epoch 144/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9645 - val_loss: 0.0743 - val_acc: 0.9653\n",
            "Epoch 145/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9645 - val_loss: 0.0679 - val_acc: 0.9649\n",
            "Epoch 146/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9647 - val_loss: 0.0733 - val_acc: 0.9655\n",
            "Epoch 147/200\n",
            "6092/6092 [==============================] - 16s 3ms/step - loss: 0.0267 - acc: 0.9646 - val_loss: 0.0704 - val_acc: 0.9652\n",
            "Epoch 148/200\n",
            "4224/6092 [===================>..........] - ETA: 4s - loss: 0.0266 - acc: 0.9647"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "77NOwzn9UaYK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "def plot_digits(*args):\n",
        "    args = [x.squeeze() for x in args]\n",
        "    n = min([x.shape[0] for x in args])\n",
        "    \n",
        "    plt.figure(figsize=(2 * n, 2 * len(args)))\n",
        "    for j in range(n):\n",
        "        for i, _ in enumerate(args):\n",
        "        # for i in range(len(args)):\n",
        "            ax = plt.subplot(len(args), n, i * n + j + 1)\n",
        "            plt.imshow(args[i][j])\n",
        "            plt.gray()\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "n = 20\n",
        "trash = 0.2\n",
        "\n",
        "answers = y_test[:n, :, :, 1:]\n",
        "imgs = x_test[:n]\n",
        "predicted = model.predict(imgs, batch_size=n)[:, :, :, 1:]\n",
        "predicted_bool = predicted > trash\n",
        "\n",
        "plot_digits(imgs, predicted, predicted_bool, answers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YdK-EQorWeEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.utils.fixes import signature\n",
        "\n",
        "y_score = model.predict(x_test, batch_size=n)[:, :, :, 1:]\n",
        "y_label = y_test[:, :, :, 1:] > 0.5\n",
        "\n",
        "y_score = y_score.reshape([y_score.size,])\n",
        "y_label = y_label.reshape([y_label.size,])\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_label, y_score)\n",
        "\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GeIJVnpyWe0r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}